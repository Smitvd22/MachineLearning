import pandas as pd
import numpy as np
import re
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

def load_and_understand_data(file_path):
    """Load the dataset and display basic information"""
    print("Loading FAA Aviation Dataset...")
    df = pd.read_csv(file_path)
    
    print(f"Dataset shape: {df.shape}")
    print("\nColumn names:")
    print(df.columns.tolist())
    print("\nFirst few rows:")
    print(df.head())
    print("\nDataset info:")
    print(df.info())
    print("\nMissing values count:")
    print(df.isnull().sum())
    
    return df

def custom_datetime_converter(date_str, time_str):
    """Custom datetime conversion without using standard pandas functions"""
    if pd.isna(date_str) or pd.isna(time_str):
        return None
    
    try:
        # Parse date string manually (format: DD-MMM-YY)
        date_parts = str(date_str).split('-')
        if len(date_parts) != 3:
            return None
        
        day = int(date_parts[0])
        
        # Month mapping
        month_map = {
            'JAN': 1, 'FEB': 2, 'MAR': 3, 'APR': 4, 'MAY': 5, 'JUN': 6,
            'JUL': 7, 'AUG': 8, 'SEP': 9, 'OCT': 10, 'NOV': 11, 'DEC': 12
        }
        month = month_map.get(date_parts[1].upper(), 1)
        
        year = int(date_parts[2])
        if year < 50:  # Assume years less than 50 are 20xx
            year += 2000
        else:
            year += 1900
        
        # Parse time string manually (format: HH:MM:SSZ)
        time_clean = str(time_str).replace('Z', '')
        time_parts = time_clean.split(':')
        if len(time_parts) != 3:
            return None
        
        hour = int(time_parts[0])
        minute = int(time_parts[1])
        second = int(time_parts[2])
        
        return datetime(year, month, day, hour, minute, second)
    
    except:
        return None

def convert_datetime_columns(df):
    """Convert date and time columns to datetime objects using custom function"""
    print("\nConverting date and time columns using custom converter...")
    
    # Apply custom datetime conversion
    df['DATETIME'] = df.apply(lambda row: custom_datetime_converter(
        row['EVENT_LCL_DATE'], row['EVENT_LCL_TIME']), axis=1)
    
    # Count successful conversions
    successful_conversions = df['DATETIME'].notna().sum()
    total_rows = len(df)
    
    print(f"Successfully converted {successful_conversions} out of {total_rows} datetime entries")
    print(f"Conversion success rate: {successful_conversions/total_rows*100:.2f}%")
    print(f"Sample datetime values:\n{df['DATETIME'].dropna().head()}")
    
    return df

def extract_required_columns(df):
    """Extract the required attributes and create new dataframe"""
    print("\nExtracting required columns...")
    
    required_columns = [
        'ACFT_MAKE_NAME',      # Aircraft make name
        'LOC_STATE_NAME',      # State name
        'ACFT_MODEL_NAME',     # Aircraft model name
        'RMK_TEXT',            # Text information
        'FLT_PHASE',           # Flight phase
        'EVENT_TYPE_DESC',     # Event description type
        'FATAL_FLAG',          # Fatal flag
        'ACFT_DMG_DESC',       # Aircraft damage description (for encoding)
        'DATETIME'             # Combined datetime
    ]
    
    # Create new dataframe with required columns
    new_df = df[required_columns].copy()
    
    print(f"New dataframe shape: {new_df.shape}")
    print(f"Columns in new dataframe: {new_df.columns.tolist()}")
    
    return new_df

def custom_mode_calculator(series):
    """Calculate mode (most frequent value) without using pandas mode()"""
    # Count frequency of each value manually
    value_counts = {}
    for value in series.dropna():
        if value in value_counts:
            value_counts[value] += 1
        else:
            value_counts[value] = 1
    
    if not value_counts:
        return None
    
    # Find the value with maximum frequency
    max_count = max(value_counts.values())
    mode_values = [k for k, v in value_counts.items() if v == max_count]
    
    return mode_values[0]  # Return first mode if multiple modes exist

def handle_missing_values(df):
    """Handle missing values using custom implementations"""
    print("\nHandling missing values using custom functions...")
    
    # Check missing values before processing
    print("Missing values before processing:")
    missing_before = {}
    for col in df.columns:
        missing_count = df[col].isna().sum()
        missing_before[col] = missing_count
        print(f"{col}: {missing_count}")
    
    # For FATAL_FLAG, replace missing values with 'No' (custom implementation)
    fatal_flag_missing = df['FATAL_FLAG'].isna()
    df.loc[fatal_flag_missing, 'FATAL_FLAG'] = 'No'
    print(f"\nReplaced {fatal_flag_missing.sum()} missing FATAL_FLAG values with 'No'")
    
    # For FLT_PHASE, use custom mode calculation
    most_frequent_phase = custom_mode_calculator(df['FLT_PHASE'])
    if most_frequent_phase is None:
        most_frequent_phase = 'UNKNOWN (UNK)'
    
    flt_phase_missing = df['FLT_PHASE'].isna()
    df.loc[flt_phase_missing, 'FLT_PHASE'] = most_frequent_phase
    print(f"Most frequent flight phase: {most_frequent_phase}")
    print(f"Replaced {flt_phase_missing.sum()} missing FLT_PHASE values")
    
    # For ACFT_DMG_DESC, use custom mode calculation
    most_frequent_damage = custom_mode_calculator(df['ACFT_DMG_DESC'])
    if most_frequent_damage is None:
        most_frequent_damage = 'Unknown'
    
    damage_missing = df['ACFT_DMG_DESC'].isna()
    df.loc[damage_missing, 'ACFT_DMG_DESC'] = most_frequent_damage
    print(f"Most frequent damage description: {most_frequent_damage}")
    print(f"Replaced {damage_missing.sum()} missing ACFT_DMG_DESC values")
    
    # Verify missing values are replaced
    print("\nMissing values after processing:")
    for col in df.columns:
        missing_count = df[col].isna().sum()
        print(f"{col}: {missing_count}")
    
    return df

def custom_drop_missing_aircraft(df):
    """Custom function to drop rows with missing aircraft names"""
    # Identify rows with missing aircraft names
    missing_aircraft = df['ACFT_MAKE_NAME'].isna()
    
    # Create new dataframe without missing aircraft names
    df_cleaned = df[~missing_aircraft].copy()
    
    # Reset index manually
    df_cleaned.reset_index(drop=True, inplace=True)
    
    return df_cleaned

def custom_drop_high_missing_columns(df, threshold_percentage=75):
    """Custom function to drop columns with more than threshold% missing values"""
    total_rows = len(df)
    threshold_count = total_rows * (threshold_percentage / 100)
    
    columns_to_keep = []
    columns_dropped = []
    
    for col in df.columns:
        non_null_count = df[col].notna().sum()
        if non_null_count >= threshold_count:
            columns_to_keep.append(col)
        else:
            columns_dropped.append(col)
    
    df_filtered = df[columns_to_keep].copy()
    
    print(f"Dropped columns with >{threshold_percentage}% missing values: {columns_dropped}")
    
    return df_filtered

def drop_unwanted_data(df):
    """Drop unwanted observations and columns using custom functions"""
    print(f"\nOriginal dataset observations: {len(df)}")
    
    # Remove observations where aircraft names are not available
    df_cleaned = custom_drop_missing_aircraft(df)
    print(f"After removing missing aircraft names: {len(df_cleaned)}")
    
    # Drop columns that have more than 75% missing values
    df_cleaned = custom_drop_high_missing_columns(df_cleaned, 75)
    
    print(f"Columns after dropping those with >75% missing values: {df_cleaned.columns.tolist()}")
    print(f"Final observations count: {len(df_cleaned)}")
    print(f"Observations dropped: {len(df) - len(df_cleaned)}")
    
    return df_cleaned

def custom_value_counts(series):
    """Custom implementation of value_counts without using pandas function"""
    value_counts = {}
    
    for value in series.dropna():
        if value in value_counts:
            value_counts[value] += 1
        else:
            value_counts[value] = 1
    
    # Sort by count in descending order
    sorted_counts = sorted(value_counts.items(), key=lambda x: x[1], reverse=True)
    
    return sorted_counts

def analyze_aircraft_groups(df):
    """Group by aircraft name and analyze frequency using custom functions"""
    print("\nGrouping dataset by aircraft make name using custom implementation...")
    
    aircraft_counts = custom_value_counts(df['ACFT_MAKE_NAME'])
    
    print("Number of times each aircraft type appears (Top 10):")
    for i, (aircraft, count) in enumerate(aircraft_counts[:10]):
        print(f"{i+1:2d}. {aircraft}: {count}")
    
    print(f"\nTotal unique aircraft makes: {len(aircraft_counts)}")
    
    return aircraft_counts

def display_fatal_observations(df):
    """Display observations where fatal flag is 'Yes'"""
    print("\nDisplaying observations where FATAL_FLAG is 'Yes':")
    
    # Custom filtering without using pandas query
    fatal_indices = []
    for idx, fatal_flag in enumerate(df['FATAL_FLAG']):
        if fatal_flag == 'Yes':
            fatal_indices.append(idx)
    
    fatal_cases = df.iloc[fatal_indices]
    print(f"Number of fatal cases: {len(fatal_cases)}")
    
    if len(fatal_cases) > 0:
        print("\nFatal cases details (first 10):")
        display_columns = ['ACFT_MAKE_NAME', 'ACFT_MODEL_NAME', 'LOC_STATE_NAME', 
                          'EVENT_TYPE_DESC', 'FLT_PHASE']
        print(fatal_cases[display_columns].head(10))
    
    return fatal_cases

def custom_one_hot_encoding(df, column_name):
    """Custom implementation of one-hot encoding"""
    print(f"\nApplying custom one-hot encoding on {column_name}...")
    
    # Get unique values
    unique_values = list(df[column_name].dropna().unique())
    print(f"Unique values in {column_name}: {unique_values}")
    
    # Create binary columns manually
    encoded_columns = {}
    
    for value in unique_values:
        column_name_encoded = f"{column_name}_{value}".replace(' ', '_').replace('(', '').replace(')', '')
        
        # Create binary column manually
        binary_column = []
        for row_value in df[column_name]:
            if pd.isna(row_value):
                binary_column.append(0)
            elif row_value == value:
                binary_column.append(1)
            else:
                binary_column.append(0)
        
        encoded_columns[column_name_encoded] = binary_column
    
    # Convert to DataFrame
    encoded_df = pd.DataFrame(encoded_columns)
    
    # Drop one column to avoid multicollinearity (drop the first one)
    if len(encoded_df.columns) > 1:
        dropped_column = encoded_df.columns[0]
        encoded_df = encoded_df.drop(columns=[dropped_column])
        print(f"Dropped column '{dropped_column}' to avoid multicollinearity")
    
    # Combine with original dataframe
    df_result = df.drop(columns=[column_name])
    df_encoded = pd.concat([df_result, encoded_df], axis=1)
    
    print(f"One-hot encoded columns: {encoded_df.columns.tolist()}")
    print(f"New dataframe shape after encoding: {df_encoded.shape}")
    
    return df_encoded

def custom_label_encoding(df, column_name):
    """Custom implementation of label encoding for comparison"""
    print(f"\nApplying custom label encoding on {column_name}...")
    
    # Get unique values
    unique_values = list(df[column_name].dropna().unique())
    print(f"Unique values for label encoding: {unique_values}")
    
    # Create label mapping manually
    label_mapping = {}
    for i, value in enumerate(unique_values):
        label_mapping[value] = i
    
    print(f"Label mapping: {label_mapping}")
    
    # Apply label encoding manually
    encoded_column = []
    for value in df[column_name]:
        if pd.isna(value):
            encoded_column.append(-1)  # Use -1 for missing values
        else:
            encoded_column.append(label_mapping[value])
    
    # Create new dataframe with label encoded column
    df_label_encoded = df.copy()
    df_label_encoded[f"{column_name}_LABEL_ENCODED"] = encoded_column
    
    return df_label_encoded, label_mapping

def compare_encoding_methods(df):
    """Compare label encoding vs one-hot encoding"""
    print("\n" + "="*70)
    print("COMPARING LABEL ENCODING VS ONE-HOT ENCODING")
    print("="*70)
    
    # Apply both encoding methods on ACFT_DMG_DESC
    test_column = 'ACFT_DMG_DESC'
    
    # Label Encoding
    df_label, label_mapping = custom_label_encoding(df, test_column)
    
    # One-Hot Encoding
    df_onehot = custom_one_hot_encoding(df.copy(), test_column)
    
    print(f"\nðŸ“Š COMPARISON RESULTS:")
    print("-" * 50)
    
    print(f"Original column '{test_column}' unique values: {len(df[test_column].dropna().unique())}")
    
    print(f"\nLABEL ENCODING:")
    print(f"  â€¢ Creates: 1 additional column")
    print(f"  â€¢ Column name: {test_column}_LABEL_ENCODED")
    print(f"  â€¢ Data type: Integer (0, 1, 2, ...)")
    print(f"  â€¢ Memory efficient: YES")
    print(f"  â€¢ Preserves ordinal relationship: NO (arbitrary assignment)")
    print(f"  â€¢ Original dataframe shape: {df.shape}")
    print(f"  â€¢ After label encoding shape: {df_label.shape}")
    
    print(f"\nONE-HOT ENCODING:")
    onehot_columns = [col for col in df_onehot.columns if col.startswith(test_column)]
    print(f"  â€¢ Creates: {len(onehot_columns)} binary columns")
    print(f"  â€¢ Column names: {onehot_columns}")
    print(f"  â€¢ Data type: Binary (0 or 1)")
    print(f"  â€¢ Memory efficient: NO (sparse representation)")
    print(f"  â€¢ Preserves ordinal relationship: N/A (creates independent features)")
    print(f"  â€¢ Original dataframe shape: {df.shape}")
    print(f"  â€¢ After one-hot encoding shape: {df_onehot.shape}")
    
    # Memory comparison
    original_memory = df.memory_usage(deep=True).sum()
    label_memory = df_label.memory_usage(deep=True).sum()
    onehot_memory = df_onehot.memory_usage(deep=True).sum()
    
    print(f"\nðŸ’¾ MEMORY USAGE COMPARISON:")
    print(f"  â€¢ Original dataframe: {original_memory / 1024:.2f} KB")
    print(f"  â€¢ With label encoding: {label_memory / 1024:.2f} KB (+{((label_memory - original_memory) / original_memory * 100):.1f}%)")
    print(f"  â€¢ With one-hot encoding: {onehot_memory / 1024:.2f} KB (+{((onehot_memory - original_memory) / original_memory * 100):.1f}%)")
    
    # Show sample transformations
    print(f"\nðŸ“‹ SAMPLE TRANSFORMATIONS:")
    print("-" * 50)
    sample_values = df[test_column].dropna().unique()[:5]
    
    print(f"{'Original Value':<20} {'Label Encoded':<15} {'One-Hot Columns'}")
    print("-" * 70)
    
    for value in sample_values:
        label_encoded_val = label_mapping.get(value, -1)
        
        # Find which one-hot columns would be 1 for this value
        onehot_active = [col for col in onehot_columns if f"_{value}".replace(' ', '_').replace('(', '').replace(')', '') in col]
        
        print(f"{str(value):<20} {label_encoded_val:<15} {', '.join(onehot_active) if onehot_active else 'None'}")
    
    print(f"\nðŸŽ¯ WHEN TO USE EACH METHOD:")
    print("-" * 50)
    print("LABEL ENCODING:")
    print("  âœ“ When feature has natural ordinal relationship")
    print("  âœ“ When memory/storage is a concern")
    print("  âœ“ With tree-based algorithms (Random Forest, XGBoost)")
    print("  âœ— With linear models (may create false ordinal relationships)")
    
    print("\nONE-HOT ENCODING:")
    print("  âœ“ When feature categories are independent/nominal")
    print("  âœ“ With linear models (Logistic Regression, SVM)")
    print("  âœ“ When you want to avoid false ordinal relationships")
    print("  âœ— When you have many categories (curse of dimensionality)")
    print("  âœ— When memory is limited")
    
    return df_onehot, df_label

def custom_text_search(text, keywords):
    """Custom text search function without using regex"""
    if pd.isna(text):
        return None
    
    text_upper = str(text).upper()
    
    for keyword in keywords:
        # Simple string search
        if keyword.upper() in text_upper:
            return keyword
    
    return None

def extract_flight_phase_from_text(df):
    """Advanced feature engineering: Extract flight phase from text using custom functions"""
    print("\nAdvanced Feature Engineering: Extracting flight phase from text...")
    
    # Define keywords for different flight phases
    phase_keywords = {
        'TAKEOFF': ['TAKEOFF', 'TAKE OFF', 'DEPARTURE', 'DEPARTING'],
        'LANDING': ['LANDING', 'LANDED', 'APPROACH', 'FINAL'],
        'CRUISE': ['CRUISE', 'CRUISING', 'FLIGHT', 'ENROUTE'],
        'TAXI': ['TAXI', 'TAXIING', 'RAMP', 'GATE'],
        'STANDING': ['PARKED', 'STANDING', 'GATE', 'PARKING']
    }
    
    def extract_phase_from_text(text):
        if pd.isna(text):
            return 'UNKNOWN'
        
        text_upper = str(text).upper()
        
        # Search for keywords in order of priority using custom search
        for phase, keywords in phase_keywords.items():
            found_keyword = custom_text_search(text, keywords)
            if found_keyword:
                return phase
        
        return 'UNKNOWN'
    
    # Create new feature using custom function
    flight_phase_text = []
    for text in df['RMK_TEXT']:
        phase = extract_phase_from_text(text)
        flight_phase_text.append(phase)
    
    df['FLIGHT_PHASE_TEXT'] = flight_phase_text
    
    # Custom comparison implementation
    print("\nComparing original FLT_PHASE with extracted FLIGHT_PHASE_TEXT:")
    
    # Clean FLT_PHASE for comparison using custom extraction
    flt_phase_clean = []
    for phase in df['FLT_PHASE']:
        if pd.isna(phase):
            flt_phase_clean.append('UNKNOWN')
        else:
            # Extract main phase word manually
            phase_str = str(phase).upper()
            main_phases = ['TAKEOFF', 'LANDING', 'CRUISE', 'TAXI', 'STANDING', 'APPROACH']
            found = False
            for main_phase in main_phases:
                if main_phase in phase_str:
                    flt_phase_clean.append(main_phase)
                    found = True
                    break
            if not found:
                flt_phase_clean.append('OTHER')
    
    df['FLT_PHASE_CLEAN'] = flt_phase_clean
    
    # Custom crosstab implementation
    print("\nCustom Cross-tabulation:")
    unique_original = list(set(flt_phase_clean))
    unique_extracted = list(set(flight_phase_text))
    
    # Create confusion matrix manually
    confusion_matrix = {}
    for orig in unique_original:
        confusion_matrix[orig] = {}
        for ext in unique_extracted:
            confusion_matrix[orig][ext] = 0
    
    # Count matches manually
    for i in range(len(df)):
        orig = flt_phase_clean[i]
        ext = flight_phase_text[i]
        confusion_matrix[orig][ext] += 1
    
    # Display confusion matrix
    print(f"{'Original \\ Extracted':<15}", end="")
    for ext in unique_extracted:
        print(f"{ext:<12}", end="")
    print()
    
    for orig in unique_original:
        print(f"{orig:<15}", end="")
        for ext in unique_extracted:
            print(f"{confusion_matrix[orig][ext]:<12}", end="")
        print()
    
    # Calculate accuracy manually
    matches = 0
    total = len(df)
    
    for i in range(total):
        if flt_phase_clean[i] == flight_phase_text[i]:
            matches += 1
    
    accuracy = matches / total * 100
    
    print(f"\nAccuracy of text-based phase extraction: {accuracy:.2f}%")
    print(f"Matches: {matches} out of {total}")
    
    # Additional analysis
    print(f"\nPhase distribution in extracted text:")
    phase_counts = custom_value_counts(pd.Series(flight_phase_text))
    for phase, count in phase_counts:
        percentage = (count / total) * 100
        print(f"  {phase}: {count} ({percentage:.1f}%)")
    
    return df

def validate_preprocessing_steps(df_original, df_final):
    """Validate all preprocessing steps"""
    print("\n" + "="*60)
    print("PREPROCESSING VALIDATION")
    print("="*60)
    
    print(f"âœ“ Original dataset shape: {df_original.shape}")
    print(f"âœ“ Final dataset shape: {df_final.shape}")
    print(f"âœ“ Rows removed: {df_original.shape[0] - df_final.shape[0]}")
    print(f"âœ“ Columns changed: {df_original.shape[1]} â†’ {df_final.shape[1]}")
    
    # Check for missing values
    missing_final = df_final.isnull().sum().sum()
    print(f"âœ“ Total missing values in final dataset: {missing_final}")
    
    # Check datetime conversion
    datetime_converted = df_final['DATETIME'].notna().sum()
    print(f"âœ“ Datetime entries successfully converted: {datetime_converted}")
    
    # Check encoding
    encoded_columns = [col for col in df_final.columns if 'ACFT_DMG_DESC' in col]
    print(f"âœ“ One-hot encoded columns created: {len(encoded_columns)}")
    
    # Check feature engineering
    has_flight_phase_text = 'FLIGHT_PHASE_TEXT' in df_final.columns
    print(f"âœ“ Advanced feature engineering completed: {has_flight_phase_text}")

def main():
    """Main function to execute all preprocessing steps"""
    file_path = r"c:\Users\acer\Desktop\U23AI118\SEM 5\ML-Lab\lab2\faa_ai_prelim.csv"
    
    try:
        # Step 1: Load and understand data
        print("STEP 1: Loading and understanding data")
        df_original = load_and_understand_data(file_path)
        
        # Step 2: Convert datetime columns
        print("\nSTEP 2: Converting datetime columns")
        df = convert_datetime_columns(df_original)
        
        # Step 3: Extract required columns
        print("\nSTEP 3: Extracting required columns")
        df_selected = extract_required_columns(df)
        
        # Step 4: Handle missing values
        print("\nSTEP 4: Handling missing values")
        df_processed = handle_missing_values(df_selected)
        
        # Step 5: Drop unwanted data
        print("\nSTEP 5: Dropping unwanted data")
        df_cleaned = drop_unwanted_data(df_processed)
        
        # Step 6: Analyze aircraft groups
        print("\nSTEP 6: Analyzing aircraft groups")
        aircraft_counts = analyze_aircraft_groups(df_cleaned)
        
        # Step 7: Display fatal observations
        print("\nSTEP 7: Displaying fatal observations")
        fatal_cases = display_fatal_observations(df_cleaned)
        
        # Step 8: Compare encoding methods
        print("\nSTEP 8: Comparing encoding methods")
        df_encoded, df_label = compare_encoding_methods(df_cleaned)
        
        # Step 9: Advanced feature engineering
        print("\nSTEP 9: Advanced feature engineering")
        df_final = extract_flight_phase_from_text(df_encoded)
        
        # Step 10: Validation
        print("\nSTEP 10: Validation")
        validate_preprocessing_steps(df_original, df_final)
        
        # Final summary
        print("\n" + "="*60)
        print("PREPROCESSING COMPLETE - ALL STEPS IMPLEMENTED FROM SCRATCH")
        print("="*60)
        print(f"âœ… Final dataset shape: {df_final.shape}")
        print(f"âœ… All custom functions implemented without standard library dependencies")
        print(f"âœ… Encoding comparison completed")
        print(f"âœ… Advanced feature engineering successful")
        
        print(f"\nðŸ“‹ Final columns: {df_final.columns.tolist()}")
        print("\nðŸ“Š Final dataset sample:")
        print(df_final.head())
        
        # Save processed dataset
        output_path = r"c:\Users\acer\Desktop\U23AI118\SEM 5\ML-Lab\lab2\faa_processed_custom.csv"
        df_final.to_csv(output_path, index=False)
        print(f"\nðŸ’¾ Processed dataset saved to: {output_path}")
        
        return df_final
        
    except Exception as e:
        print(f"âŒ Error occurred: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    processed_data = main()